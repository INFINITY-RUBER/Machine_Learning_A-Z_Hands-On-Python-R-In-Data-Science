corpus.tmp <- tm_map(corpus.tmp, removeWords, v_stopwords)
corpus.tmp <- tm_map(corpus.tmp, removeNumbers)
return(corpus.tmp)
}
# Text transformations
cleanCorpus <- function(corpus){
corpus.tmp <- tm_map(corpus, removePunctuation)
corpus.tmp <- tm_map(corpus.tmp, stripWhitespace)
corpus.tmp <- tm_map(corpus.tmp, content_transformer(tolower))
v_stopwords <- c(stopwords("english"), c("thats","weve","hes","theres","ive","im",
"will","can","cant","dont","youve","us",
"youre","youll","theyre","whats","didnt"))
corpus.tmp <- tm_map(corpus.tmp, removeWords, v_stopwords)
corpus.tmp <- tm_map(corpus.tmp, removeNumbers)
return(corpus.tmp)
}
# Most frequent terms
frequentTerms <- function(text){
s.cor <- VCorpus(VectorSource(text))
s.cor.cl <- cleanCorpus(s.cor)
s.tdm <- TermDocumentMatrix(s.cor.cl)
s.tdm <- removeSparseTerms(s.tdm, 0.999)
m <- as.matrix(s.tdm)
word_freqs <- sort(rowSums(m), decreasing=TRUE)
dm <- data.frame(word=names(word_freqs), freq=word_freqs)
return(dm)
}
# Define bigram tokenizer
tokenizer2  <- function(x){
NGramTokenizer(x, Weka_control(min=2, max=2))
}
# Define trigram tokenizer
tokenizer3  <- function(x){
NGramTokenizer(x, Weka_control(min=3, max=3))
}
# Most frequent bigrams
frequentBigrams <- function(text){
s.cor <- VCorpus(VectorSource(text))
s.cor.cl <- cleanCorpus(s.cor)
s.tdm <- TermDocumentMatrix(s.cor.cl, control=list(tokenize=tokenizer2))
s.tdm <- removeSparseTerms(s.tdm, 0.999)
m <- as.matrix(s.tdm)
word_freqs <- sort(rowSums(m), decreasing=TRUE)
dm <- data.frame(word=names(word_freqs), freq=word_freqs)
return(dm)
}
# Most frequent trigrams
frequentTrigrams <- function(text){
s.cor <- VCorpus(VectorSource(text))
s.cor.cl <- cleanCorpus(s.cor)
s.tdm <- TermDocumentMatrix(s.cor.cl, control=list(tokenize=tokenizer3))
s.tdm <- removeSparseTerms(s.tdm, 0.999)
m <- as.matrix(s.tdm)
word_freqs <- sort(rowSums(m), decreasing=TRUE)
dm <- data.frame(word=names(word_freqs), freq=word_freqs)
return(dm)
}
# Top 15 characters with more dialogues (absolute values)
scripts %>%
count(char) %>%
arrange(desc(n)) %>%
slice(1:15) %>%
ggplot(aes(x=reorder(char, n), y=n)) +
geom_bar(stat="identity", aes(fill=n), show.legend=FALSE) +
geom_label(aes(label=n)) +
scale_fill_gradient(low="paleturquoise", high="paleturquoise4") +
labs(x="Character", y="Lines of dialogue",
title="Lines of dialogue per character (absolute values)") +
coord_flip() +
theme_bw()
# Image in the visualization
image <- image_read("../input/the-lord-of-the-rings-figures/gollum.gif")
install.packages("memery")
install.packages("memery")
# Load libraries
library(tidyverse)
library(tm)
library(wordcloud)
library(wordcloud2)
library(reshape2)
library(RWeka)
install.packages("RWeka")
library(RWeka)
library(knitr)
library(gridExtra)
library(grid)
library(magick)
install.packages("magick")
library(grid)
library(magick)
install.packages("magick")
library(grid)
library(magick)
library(memery)
install.packages("memery")
library(magick)
library(memery)
library(ggimage)
library(memery)
install.packages("memery")
install.packages("cowplot")
install.packages("memery")
# Top 15 characters with more dialogues (relative values)
scripts %>%
count(char) %>%
arrange(desc(n)) %>%
slice(1:15) %>%
mutate(Percentage=n/nrow(scripts)) %>%
ggplot(aes(x=reorder(char, Percentage), y=Percentage)) +
geom_bar(stat="identity", aes(fill=Percentage), show.legend=FALSE) +
geom_label(aes(label=paste0(round(Percentage*100, 2), "%"))) +
scale_y_continuous(labels=scales::percent) +
scale_fill_gradient(low="paleturquoise", high="paleturquoise4") +
labs(x="Character", y="Lines of dialogue (%)",
title="Lines of dialogue per character (relative values)") +
coord_flip() +
theme_bw()
# Image in the visualization
image <- image_read("../input/the-lord-of-the-rings-figures/sauron.png")
grid.raster(image, x=0.85, y=0.26, height=0.34)
# Top 15 characters with more dialogues (relative values)
scripts %>%
count(char) %>%
arrange(desc(n)) %>%
slice(1:15) %>%
mutate(Percentage=n/nrow(scripts)) %>%
ggplot(aes(x=reorder(char, Percentage), y=Percentage)) +
geom_bar(stat="identity", aes(fill=Percentage), show.legend=FALSE) +
geom_label(aes(label=paste0(round(Percentage*100, 2), "%"))) +
scale_y_continuous(labels=scales::percent) +
scale_fill_gradient(low="paleturquoise", high="paleturquoise4") +
labs(x="Character", y="Lines of dialogue (%)",
title="Lines of dialogue per character (relative values)") +
coord_flip() +
theme_bw()
# Image in the visualization
image <- image_read("./unnamed-chunk-15-1.pngsauron.png")
# Image in the visualization
image <- image_read("./unnamed-chunk-15-1.pngsauron.png")
grid.raster(image, x=0.85, y=0.26, height=0.34)
install.packages(c("ggimage", "memery", "tidytext"))
# Text transformations
cleanCorpus <- function(corpus){
corpus.tmp <- tm_map(corpus, removePunctuation)
corpus.tmp <- tm_map(corpus.tmp, stripWhitespace)
corpus.tmp <- tm_map(corpus.tmp, content_transformer(tolower))
v_stopwords <- c(stopwords("english"), c("thats","weve","hes","theres","ive","im",
"will","can","cant","dont","youve","us",
"youre","youll","theyre","whats","didnt"))
corpus.tmp <- tm_map(corpus.tmp, removeWords, v_stopwords)
corpus.tmp <- tm_map(corpus.tmp, removeNumbers)
return(corpus.tmp)
}
# Most frequent terms
frequentTerms <- function(text){
s.cor <- VCorpus(VectorSource(text))
s.cor.cl <- cleanCorpus(s.cor)
s.tdm <- TermDocumentMatrix(s.cor.cl)
s.tdm <- removeSparseTerms(s.tdm, 0.999)
m <- as.matrix(s.tdm)
word_freqs <- sort(rowSums(m), decreasing=TRUE)
dm <- data.frame(word=names(word_freqs), freq=word_freqs)
return(dm)
}
# Define bigram tokenizer
tokenizer2  <- function(x){
NGramTokenizer(x, Weka_control(min=2, max=2))
}
# Define trigram tokenizer
tokenizer3  <- function(x){
NGramTokenizer(x, Weka_control(min=3, max=3))
}
# Most frequent bigrams
frequentBigrams <- function(text){
s.cor <- VCorpus(VectorSource(text))
s.cor.cl <- cleanCorpus(s.cor)
s.tdm <- TermDocumentMatrix(s.cor.cl, control=list(tokenize=tokenizer2))
s.tdm <- removeSparseTerms(s.tdm, 0.999)
m <- as.matrix(s.tdm)
word_freqs <- sort(rowSums(m), decreasing=TRUE)
dm <- data.frame(word=names(word_freqs), freq=word_freqs)
return(dm)
}
# Most frequent trigrams
frequentTrigrams <- function(text){
s.cor <- VCorpus(VectorSource(text))
s.cor.cl <- cleanCorpus(s.cor)
s.tdm <- TermDocumentMatrix(s.cor.cl, control=list(tokenize=tokenizer3))
s.tdm <- removeSparseTerms(s.tdm, 0.999)
m <- as.matrix(s.tdm)
word_freqs <- sort(rowSums(m), decreasing=TRUE)
dm <- data.frame(word=names(word_freqs), freq=word_freqs)
return(dm)
}
# Top 15 characters with more dialogues (absolute values)
scripts %>%
count(char) %>%
arrange(desc(n)) %>%
slice(1:15) %>%
ggplot(aes(x=reorder(char, n), y=n)) +
geom_bar(stat="identity", aes(fill=n), show.legend=FALSE) +
geom_label(aes(label=n)) +
scale_fill_gradient(low="paleturquoise", high="paleturquoise4") +
labs(x="Character", y="Lines of dialogue",
title="Lines of dialogue per character (absolute values)") +
coord_flip() +
theme_bw()
# Image in the visualization
image <- image_read("../input/the-lord-of-the-rings-figures/gollum.gif")
# Top 15 characters with more dialogues (relative values)
scripts %>%
count(char) %>%
arrange(desc(n)) %>%
slice(1:15) %>%
mutate(Percentage=n/nrow(scripts)) %>%
ggplot(aes(x=reorder(char, Percentage), y=Percentage)) +
geom_bar(stat="identity", aes(fill=Percentage), show.legend=FALSE) +
geom_label(aes(label=paste0(round(Percentage*100, 2), "%"))) +
scale_y_continuous(labels=scales::percent) +
scale_fill_gradient(low="paleturquoise", high="paleturquoise4") +
labs(x="Character", y="Lines of dialogue (%)",
title="Lines of dialogue per character (relative values)") +
coord_flip() +
theme_bw()
# Image in the visualization
image <- image_read("../input/the-lord-of-the-rings-figures/sauron.png")
# Lines of dialogue per character
plot1 <- scripts %>%
filter(char %in% c("FRODO", "SAM", "GANDALF", "ARAGORN", "PIPPIN",
"MERRY", "GOLLUM", "GIMLI", "THEODEN", "FARAMIR",
"EOWYN", "LEGOLAS", "SMEAGOL", "TREEBEARD", "BILBO")) %>%
count(movie, char) %>%
ggplot(aes(x=reorder(char, n, sum), y=n)) +
geom_bar(stat="identity", aes(fill=movie)) +
labs(x="Character", y="Lines of dialogue",
title="Lines of dialogue per character") +
theme_bw() +
theme(legend.position="bottom",
legend.title=element_blank()) +
coord_flip()
# Lines of dialogue per movie (%)
plot2 <- scripts %>%
count(movie) %>%
mutate(Percentage=paste0(round(n/sum(n)*100, 2), "%")) %>%
ggplot(aes(x=factor(1), y=n, fill=movie)) +
geom_bar(stat="identity", width=1, size=1, color="white", show.legend=FALSE) +
coord_polar(theta="y") +
labs(title="Lines of dialogue per movie (%)") +
theme_void() +
geom_text(aes(label=Percentage),
position=position_stack(vjust = 0.5))
# Subplot
grid.arrange(plot1, plot2, ncol=2)
# Transform the text to a tidy data structure with one token per row
tokens <- scripts %>%
mutate(dialogue=as.character(scripts$dialog)) %>%
unnest_tokens(word, dialogue)
# Transform the text to a tidy data structure with one token per row
tokens <- scripts %>%
mutate(dialogue=as.character(scripts$dialog)) %>%
unnest_tokens(word, dialogue)
# Transform the text to a tidy data structure with one token per row
tokens <- scripts %>%
mutate(dialogue=as.character(scripts$dialog)) %>%
unnest_tokens(word, dialogue)
# Positive and negative words
tokens %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment, sort=TRUE) %>%
acast(word ~ sentiment, value.var="n", fill=0) %>%
comparison.cloud(colors=c("#F8766D", "#00BFC4"), max.words=100)
# Frequency associated with each sentiment
sentiments <- tokens %>%
inner_join(get_sentiments("nrc")) %>%
count(sentiment, sort=TRUE)
install.packages("tokenizers")
# Positive and negative words
tokens %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment, sort=TRUE) %>%
acast(word ~ sentiment, value.var="n", fill=0) %>%
comparison.cloud(colors=c("#F8766D", "#00BFC4"), max.words=100)
# Positive and negative words
tokens %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment, sort=TRUE) %>%
acast(word ~ sentiment, value.var="n", fill=0) %>%
comparison.cloud(colors=c("#F8766D", "#00BFC4"), max.words=100)
# Transform the text to a tidy data structure with one token per row
tokens <- scripts %>%
mutate(dialogue=as.character(scripts$dialog)) %>%
unnest_tokens(word, dialogue)
# Lines of dialogue per character
plot1 <- scripts %>%
filter(char %in% c("FRODO", "SAM", "GANDALF", "ARAGORN", "PIPPIN",
"MERRY", "GOLLUM", "GIMLI", "THEODEN", "FARAMIR",
"EOWYN", "LEGOLAS", "SMEAGOL", "TREEBEARD", "BILBO")) %>%
count(movie, char) %>%
ggplot(aes(x=reorder(char, n, sum), y=n)) +
geom_bar(stat="identity", aes(fill=movie)) +
labs(x="Character", y="Lines of dialogue",
title="Lines of dialogue per character") +
theme_bw() +
theme(legend.position="bottom",
legend.title=element_blank()) +
coord_flip()
# Lines of dialogue per movie (%)
plot2 <- scripts %>%
count(movie) %>%
mutate(Percentage=paste0(round(n/sum(n)*100, 2), "%")) %>%
ggplot(aes(x=factor(1), y=n, fill=movie)) +
geom_bar(stat="identity", width=1, size=1, color="white", show.legend=FALSE) +
coord_polar(theta="y") +
labs(title="Lines of dialogue per movie (%)") +
theme_void() +
geom_text(aes(label=Percentage),
position=position_stack(vjust = 0.5))
# Subplot
grid.arrange(plot1, plot2, ncol=2)
setwd("~/Documentos/CURSOS/CURSOS UDEMY/Curso_de_Machine_learning_A_Z_Pytho_R/cogidos/Machine Learning A-Z (Codes and Datasets)/Part 6 - Reinforcement Learning/Section 32 - Upper Confidence Bound (UCB)/R")
# Upper Confidence Bound
# Importing the dataset
dataset = read.csv('Ads_CTR_Optimisation.csv')
# Implementing UCB
N = 10000
d = 10
ads_selected = integer(0)
numbers_of_selections = integer(d)
sums_of_rewards = integer(d)
total_reward = 0
for (n in 1:N) {
ad = 0
max_upper_bound = 0
for (i in 1:d) {
if (numbers_of_selections[i] > 0) {
average_reward = sums_of_rewards[i] / numbers_of_selections[i]
delta_i = sqrt(3/2 * log(n) / numbers_of_selections[i])
upper_bound = average_reward + delta_i
} else {
upper_bound = 1e400
}
if (upper_bound > max_upper_bound) {
max_upper_bound = upper_bound
ad = i
}
}
ads_selected = append(ads_selected, ad)
numbers_of_selections[ad] = numbers_of_selections[ad] + 1
reward = dataset[n, ad]
sums_of_rewards[ad] = sums_of_rewards[ad] + reward
total_reward = total_reward + reward
}
# Upper Confidence Bound
# Importing the dataset
dataset = read.csv('Ads_CTR_Optimisation.csv')
# Implementing UCB
N = 10000
d = 10
ads_selected = integer(0)
numbers_of_selections = integer(d)
sums_of_rewards = integer(d)
total_reward = 0
for (n in 1:N) {
ad = 0
max_upper_bound = 0
for (i in 1:d) {
if (numbers_of_selections[i] > 0) {
average_reward = sums_of_rewards[i] / numbers_of_selections[i]
delta_i = sqrt(3/2 * log(n) / numbers_of_selections[i])
upper_bound = average_reward + delta_i
} else {
upper_bound = 1e400
}
if (upper_bound > max_upper_bound) {
max_upper_bound = upper_bound
ad = i
}
}
ads_selected = append(ads_selected, ad)
numbers_of_selections[ad] = numbers_of_selections[ad] + 1
reward = dataset[n, ad]
sums_of_rewards[ad] = sums_of_rewards[ad] + reward
total_reward = total_reward + reward
}
hist(ads_selected,
col = 'blue',
main = 'Histogram of ads selections',
xlab = 'Ads',
ylab = 'Number of times each ad was selected')
# Visualising the results
hist(ads_selected,
col = 'blue',
main = 'Histogram of ads selections',
xlab = 'Ads',
ylab = 'Number of times each ad was selected')
# Visualising the results
hist(ads_selected,
col = 'blue',
main = 'Histogram of ads selections',
xlab = 'Ads',
ylab = 'Number of times each ad was selected')
# Thompson Sampling
# Importing the dataset
dataset = read.csv('Ads_CTR_Optimisation.csv')
# Implementing Thompson Sampling
N = 10000
d = 10
ads_selected = integer(0)
numbers_of_rewards_1 = integer(d)
numbers_of_rewards_0 = integer(d)
total_reward = 0
for (n in 1:N) {
ad = 0
max_random = 0
for (i in 1:d) {
random_beta = rbeta(n = 1,
shape1 = numbers_of_rewards_1[i] + 1,
shape2 = numbers_of_rewards_0[i] + 1)
if (random_beta > max_random) {
max_random = random_beta
ad = i
}
}
ads_selected = append(ads_selected, ad)
reward = dataset[n, ad]
if (reward == 1) {
numbers_of_rewards_1[ad] = numbers_of_rewards_1[ad] + 1
} else {
numbers_of_rewards_0[ad] = numbers_of_rewards_0[ad] + 1
}
total_reward = total_reward + reward
}
# Visualising the results
hist(ads_selected,
col = 'blue',
main = 'Histogram of ads selections',
xlab = 'Ads',
ylab = 'Number of times each ad was selected')
# Thompson Sampling
# Importing the dataset
dataset = read.csv('Ads_CTR_Optimisation.csv')
# Implementing Thompson Sampling
N = 10000
d = 10
ads_selected = integer(0)
numbers_of_rewards_1 = integer(d)
numbers_of_rewards_0 = integer(d)
total_reward = 0
for (n in 1:N) {
ad = 0
max_random = 0
for (i in 1:d) {
random_beta = rbeta(n = 1,
shape1 = numbers_of_rewards_1[i] + 1,
shape2 = numbers_of_rewards_0[i] + 1)
if (random_beta > max_random) {
max_random = random_beta
ad = i
}
}
ads_selected = append(ads_selected, ad)
reward = dataset[n, ad]
if (reward == 1) {
numbers_of_rewards_1[ad] = numbers_of_rewards_1[ad] + 1
} else {
numbers_of_rewards_0[ad] = numbers_of_rewards_0[ad] + 1
}
total_reward = total_reward + reward
}
# Visualising the results
hist(ads_selected,
col = 'blue',
main = 'Histogram of ads selections',
xlab = 'Ads',
ylab = 'Number of times each ad was selected')
# Thompson Sampling
# Importing the dataset
dataset = read.csv('Ads_CTR_Optimisation.csv')
# Implementing Thompson Sampling
N = 10000
d = 10
ads_selected = integer(0)
numbers_of_rewards_1 = integer(d)
numbers_of_rewards_0 = integer(d)
total_reward = 0
for (n in 1:N) {
ad = 0
max_random = 0
for (i in 1:d) {
random_beta = rbeta(n = 1,
shape1 = numbers_of_rewards_1[i] + 1,
shape2 = numbers_of_rewards_0[i] + 1)
if (random_beta > max_random) {
max_random = random_beta
ad = i
}
}
ads_selected = append(ads_selected, ad)
reward = dataset[n, ad]
if (reward == 1) {
numbers_of_rewards_1[ad] = numbers_of_rewards_1[ad] + 1
} else {
numbers_of_rewards_0[ad] = numbers_of_rewards_0[ad] + 1
}
total_reward = total_reward + reward
}
# Visualising the results
hist(ads_selected,
col = 'blue',
main = 'Histogram of ads selections',
xlab = 'Ads',
ylab = 'Number of times each ad was selected')
